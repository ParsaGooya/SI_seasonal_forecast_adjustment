{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from data_locations import LOC_FORECASTS_SI, LOC_OBSERVATIONS_SI\n",
    "from tqdm import tqdm\n",
    "# from preprocessing import get_anomalies #, get_climatology \n",
    "from preprocessing import align_data_and_targets\n",
    "from preprocessing import AnomaliesScaler, Detrender, Standardizer, Normalizer, PreprocessingPipeline, Spatialnanremove, create_mask\n",
    "from preprocessing import detrend, standardize, normalize, linear_debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var     = 'SICN'\n",
    "fct_set = 'canesm5'    \n",
    "NPSproj = True\n",
    "if NPSproj:\n",
    "    crs = 'NPS'  \n",
    "else: \n",
    "    crs = '1x1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify data directories --specified in data_locations.py\n",
    "data_dir_forecast = LOC_FORECASTS_SI\n",
    "data_dir_obs = glob.glob(LOC_OBSERVATIONS_SI+ f'/NASA*{crs}*.nc')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Load observations\")\n",
    "obs_in = xr.open_dataset(data_dir_obs)[var].clip(0,1)\n",
    "print(\"Load forecasts\") \n",
    "ls = [xr.open_dataset(glob.glob(LOC_FORECASTS_SI + f'/*_initial_month_{intial_month}_*{crs}*.nc')[0])['SICN'].mean('ensembles').load() for intial_month in range(1,13) ]\n",
    "ds_in = xr.concat(ls, dim = 'time').sortby('time').clip(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_dir = '/space/hall5/sitestore/eccc/crd/ccrn/users/rpg002/output/SI/Full/results/CNN/run_set_1/N15_M12_v1_North_batch100_e20/'\n",
    "# ds_in =  xr.open_mfdataset(str(Path(out_dir, \"*.nc\")), combine='nested', concat_dim='time').load()['nn_adjusted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fct, obs = align_data_and_targets(ds_in, obs_in, 12)\n",
    "if not fct.time.equals(obs.time):\n",
    "    fct = fct.sel(time = obs.time)\n",
    "assert fct.time.equals(obs.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0_test = 2006\n",
    "y1_test = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_frnt_gen     = '/space/hall5/sitestore/eccc/crd/ccrn/users/rpg002/output/SI/Full/results/NASA'\n",
    "\n",
    "dir_badj_out = f'{dir_frnt_gen}/Bias_Adjusted/'\n",
    "dir_tadj_out = f'{dir_frnt_gen}/Trend_Adjusted/'\n",
    "dir_ladj_out = f'{dir_frnt_gen}/Linear_Adjusted/'\n",
    "\n",
    "Path(dir_badj_out).mkdir(parents=True,exist_ok=True)\n",
    "Path(dir_tadj_out).mkdir(parents=True,exist_ok=True)\n",
    "Path(dir_ladj_out).mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_climatology(ds):\n",
    "    return xr.concat([ds.isel(time = np.arange(0,len(ds.time),12) + init_month).mean('time') for init_month in range(12) ], dim = 'init_month').assign_coords(init_month = np.arange(1,13))\n",
    "\n",
    "def calculate_anomalies(ds, monthly_climatology):\n",
    "    return xr.concat([ds.where(np.mod(ds.time,100) == init_month, drop = True) - monthly_climatology.sel(init_month = init_month).values for init_month in range(1,13)], dim = 'time').sortby('time')\n",
    "\n",
    "def bias_adj(fct, fct_clim, obs_clim):\n",
    "        fct_anom  = calculate_anomalies(fct, fct_clim)\n",
    "        return xr.concat([fct_anom.where(np.mod(fct_anom.time,100) == init_month, drop = True) + obs_clim.sel(init_month = init_month).values for init_month in range(1,13)], dim = 'time').sortby('time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias adjustment: test year 1995\n",
      "bias adjustment: test year 1996\n",
      "bias adjustment: test year 1997\n",
      "bias adjustment: test year 1998\n",
      "bias adjustment: test year 1999\n",
      "bias adjustment: test year 2000\n",
      "bias adjustment: test year 2001\n",
      "bias adjustment: test year 2002\n",
      "bias adjustment: test year 2003\n",
      "bias adjustment: test year 2004\n",
      "bias adjustment: test year 2005\n",
      "bias adjustment: test year 2006\n",
      "bias adjustment: test year 2007\n",
      "bias adjustment: test year 2008\n",
      "bias adjustment: test year 2009\n",
      "bias adjustment: test year 2010\n",
      "bias adjustment: test year 2011\n",
      "bias adjustment: test year 2012\n",
      "bias adjustment: test year 2013\n",
      "bias adjustment: test year 2014\n",
      "bias adjustment: test year 2015\n",
      "bias adjustment: test year 2016\n",
      "bias adjustment: test year 2017\n",
      "bias adjustment: test year 2018\n",
      "bias adjustment: test year 2019\n"
     ]
    }
   ],
   "source": [
    "#bias adjustment\n",
    "\n",
    "\n",
    "test_years = np.arange(y0_test, y1_test + 1 )\n",
    "\n",
    "ls = []\n",
    "\n",
    "for y_idx, test_year in enumerate(test_years):\n",
    "    print(f\"bias adjustment: test year {test_year}\") \n",
    "    ds_full  = fct.where(fct.time < 100* (test_year + 1), drop  =True)  ### or fct_anom\n",
    "    obs_full = obs.where(obs.time < 100* (test_year + 1), drop  =True) ### or obs_anom\n",
    "    ds_base = ds_full.where(ds_full.time < 100 * test_year, drop  =True)\n",
    "    obs_base = obs_full.where(obs_full.time < 100 * test_year, drop  =True)\n",
    "    train_mask = create_mask(ds_base)  \n",
    "    preprocessing_mask = np.broadcast_to(train_mask[...,None,None], ds_base.shape)\n",
    "    fct_clim = calculate_climatology(ds_base.where(~preprocessing_mask))\n",
    "    obs_clim = calculate_climatology(obs_base.where(~preprocessing_mask))\n",
    "    fct_debiased = bias_adj(ds_full , fct_clim , obs_clim)\n",
    "    ls.append(fct_debiased.where(fct_debiased.time > test_year*100 ,drop = True) )\n",
    "    \n",
    "fct_debiased = xr.concat(ls,\n",
    "                         dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fct_debiased' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfct_debiased\u001b[49m\u001b[38;5;241m.\u001b[39mto_netcdf(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdir_badj_out\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/bias_adjusted_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my0_test\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my1_test\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_NPSproj.nc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fct_debiased' is not defined"
     ]
    }
   ],
   "source": [
    "fct_debiased.to_netcdf(f'{dir_badj_out}/bias_adjusted_{y0_test}-{y1_test}_NPSproj.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trend adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculator_detrender(ds_base):\n",
    "    detrender = Detrender(trend_dim='time', deg=1)\n",
    "    detrenders = [ detrender.fit(ds_base.isel(time = np.arange(0,len(ds_base.time),12) + init_month)) for init_month in range(12) ]   \n",
    "    return detrenders\n",
    "\n",
    "\n",
    "def trend_adj(ds, fct_detrender, obs_detrender): \n",
    "    fct_detrended = [fct_detrender[init_month].transform(ds.isel(time = np.arange(0,len(ds.time),12) + init_month),  remove_intercept=True) for init_month in range(12)]  \n",
    "    return xr.concat([obs_detrender[init_month].inverse_transform(fct_detrended[init_month],  add_intercept=True) for init_month in range(12)] , dim = 'time').sortby('time')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trend adjustment: test year 1995\n",
      "trend adjustment: test year 1996\n",
      "trend adjustment: test year 1997\n",
      "trend adjustment: test year 1998\n",
      "trend adjustment: test year 1999\n",
      "trend adjustment: test year 2000\n",
      "trend adjustment: test year 2001\n",
      "trend adjustment: test year 2002\n",
      "trend adjustment: test year 2003\n",
      "trend adjustment: test year 2004\n",
      "trend adjustment: test year 2005\n",
      "trend adjustment: test year 2006\n",
      "trend adjustment: test year 2007\n",
      "trend adjustment: test year 2008\n",
      "trend adjustment: test year 2009\n",
      "trend adjustment: test year 2010\n",
      "trend adjustment: test year 2011\n",
      "trend adjustment: test year 2012\n",
      "trend adjustment: test year 2013\n",
      "trend adjustment: test year 2014\n",
      "trend adjustment: test year 2015\n",
      "trend adjustment: test year 2016\n",
      "trend adjustment: test year 2017\n",
      "trend adjustment: test year 2018\n",
      "trend adjustment: test year 2019\n"
     ]
    }
   ],
   "source": [
    "#trend adjustment\n",
    "\n",
    "test_years = np.arange(y0_test, y1_test + 1 )\n",
    "\n",
    "ls = []\n",
    "\n",
    "for y_idx, test_year in tqdm(enumerate(test_years)):\n",
    "    \n",
    "    print(f\"trend adjustment: test year {test_year}\")\n",
    "\n",
    "    ds_full  = fct.where(fct.time < 100* (test_year + 1), drop  =True)  ### or fct_anom\n",
    "    obs_full = obs.where(obs.time < 100* (test_year + 1), drop  =True) ### or obs_anom\n",
    "\n",
    "    ds_base = ds_full.where(ds_full.time < 100 * test_year, drop  =True)\n",
    "    obs_base = obs_full.where(obs_full.time < 100 * test_year, drop  =True)\n",
    "    \n",
    "    train_mask = create_mask(ds_base)\n",
    "    preprocessing_mask = np.broadcast_to(train_mask[...,None,None], ds_base.shape)\n",
    " \n",
    "    fct_detrender = calculator_detrender(ds_base.where(~preprocessing_mask))\n",
    "    obs_detrender = calculator_detrender(obs_base.where(~preprocessing_mask))\n",
    "\n",
    "    fct_tr_adj = trend_adj(ds_full , fct_detrender , obs_detrender)\n",
    "    \n",
    "    ls.append(fct_tr_adj.where(fct_tr_adj.time > test_year*100 ,drop = True))\n",
    "\n",
    "fct_trendcorr = xr.concat(ls, dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fct_trendcorr.to_netcdf(f'{dir_tadj_out}/trend_adjusted_{y0_test}-{y1_test}.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
